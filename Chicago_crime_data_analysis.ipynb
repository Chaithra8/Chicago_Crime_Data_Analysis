{"cells":[{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions.{col, to_date}\n\n// To define user defined schema and specify datatype\nval schema = StructType(Array(\n    StructField(\"ID\",IntegerType,true),\n    StructField(\"Case Number\",StringType,true),\n    StructField(\"Date\",StringType,true),\n    StructField(\"Block\",StringType,true),\n    StructField(\"IUCR\", StringType,true),\n    StructField(\"Primary Type\",StringType,true),\n    StructField(\"Description\",StringType,true),\n    StructField(\"Location Description\",StringType,true),\n    StructField(\"Arrest\",BooleanType,true),\n    StructField(\"Domestic\", BooleanType,true),\n    StructField(\"Beat\",IntegerType,true),\n    StructField(\"District\",IntegerType,true),\n    StructField(\"Ward\",IntegerType,true),\n    StructField(\"Community Area\",IntegerType,true),\n    StructField(\"FBI Code\",StringType,true),\n    StructField(\"X Coordinate\",IntegerType,true),\n    StructField(\"Y Coordinate\",IntegerType,true),\n    StructField(\"Year\",IntegerType,true),\n    StructField(\"Updated On\",StringType,true),\n    StructField(\"Latitude\",DoubleType,true),\n    StructField(\"Longitude\",DoubleType,true),\n    StructField(\"Location\",StringType,true)\n  ))\n\nval crimeDF = spark.read.format(\"com.databricks.spark.csv\").option(\"header\",true).schema(schema).csv(\"/FileStore/tables/Crimes___2001_to_Present.csv\")\ncrimeDF.printSchema()\ncrimeDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c69f4b43-7bbf-4f6a-af6d-65158a6ebd7c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n\n//   Create a view or table\ncrimeDF.createOrReplaceTempView(\"crime_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc25a1de-7ffe-429d-a12b-31e4ad84a9ba","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Problem 1: How the number of various crimes changed over time in Chicago?\n\ndisplay(spark.sql(\"SELECT `Primary Type`,Year, COUNT(*) AS `Crime Count` FROM crime_table GROUP BY Year,`Primary Type` ORDER BY Year, `Primary Type` DESC\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b75e7878-3a36-4305-a31a-8fef610a0685","inputWidgets":{},"title":"Problem 1: How the number of various crimes changed over time in Chicago?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Problem 2: How have the number arrests corresponding to the crimes changed over time in Chicago?\n\ndisplay(spark.sql(\"select Year,count(*) from crime_table where Arrest='true' group by Year order by Year\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"5761e47d-15fc-4322-aeeb-3fafe6d4b28a","inputWidgets":{},"title":"Problem 2: How have the number arrests corresponding to the crimes changed over time in Chicago?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// Problem 3: Which contiguous months show largest variation in crime?\n\n//  Step 1: Extracting months from Date column and grouping the crimes per month\nval month_count=spark.sql(\"select substring(Date,0,2) as Month_val,count(*) as noofcrimes from crime_table group by Month_val order by Month_val\")\n\n// Step 2: Using lag function to find variation in crime compared with months\nmonth_count.createOrReplaceTempView(\"month_counts\")\nval finalresult7=spark.sql(\"\"\"select *,abs(previouscount-noofcrimes) as variation_value from(select *,LAG(noofcrimes,1) over (order by Month_val) as previouscount from month_counts) where previouscount is not null\"\"\")\n\n// Step 3: Displaying month having highest variation in crime\nfinalresult7.createOrReplaceTempView(\"result\")\nspark.sql(\"select Month_val as Month_with_highest_variation from result where variation_value=(select max(variation_value) as high_count from result)\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"156aaa16-0e26-48dd-ba4f-2b905397fb74","inputWidgets":{},"title":"Problem 3: Which contiguous months show largest variation in crime?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\n%scala\n\n// Problem 4: Which quarter was the most peaceful compared to the previous quarter?\n\n\nimport org.apache.spark.sql.functions.{col, to_date}\n// Step 1: Extracting date in string format and count of crimes for all the dates\nval datedf=spark.sql(\"SELECT substring(Date,0,10) as Date_val,count(ID) as crimeCountOfDate  from crime_table group by Date_val\")\n\n// Step 2: converting dates from string to date format\nval newDate=datedf.select(col(\"Date_val\"),\n    to_date(col(\"Date_val\"),\"MM/dd/yyyy\").as(\"finalDate\"),col(\"crimeCountOfDate\"))\n\n// Step 3: Grouping by quarters and finding the sum of crime counts per quarter\nnewDate.createOrReplaceTempView(\"newDate\")\nval resultdf=spark.sql(\"select quarter(finalDate) as quarters,sum(crimeCountOfDate) as totalCrimeCountPerQuarter from newDate group by quarters order by quarters\")\n\n// Step 4: using lag function to compare the variation of crime counts with the previous quarter\nresultdf.createOrReplaceTempView(\"resultdf\")\nval result=spark.sql(\"\"\"\nselect *,abs(previous_count-totalCrimeCountPerQuarter)as count_varition_per_quarter from (\nselect *,LAG(totalCrimeCountPerQuarter) OVER (ORDER BY quarters) as previous_count from resultdf)as a where previous_count is not null\"\"\")\n\n// Step 5: finding the peaceful quarter compared to previous quarter\nresult.createOrReplaceTempView(\"crime_result\")\nspark.sql(\"select quarters from crime_result where count_varition_per_quarter=(select min(count_varition_per_quarter) from crime_result)\").show\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"4d0a4d7a-e7f6-4a06-8a1c-86f28174b781","inputWidgets":{},"title":"Problem 4: Which quarter was the most peaceful compared to the previous quarter?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// Problem 5: Are there any trends in the crimes being committed?\n\n// Step 1: display total number of crimes in each year\nval displayYear=spark.sql(\"select Year as Year,count(*) as countOfCrimesInYear from crime_table group by Year order by Year\")\n\n// Step 2: Using the lag function to get the variation in crime rates for each year\ndisplayYear.createOrReplaceTempView(\"year_counts\")\nval trendresult=spark.sql(\"\"\"select Year,abs(previouscount-countOfCrimesInYear) as variation_value from(select Year,countOfCrimesInYear,LAG(countOfCrimesInYear,1)  over (order by Year) as previouscount from year_counts) where previouscount is not null\"\"\")\n\n// Step 3: finding the years which has minimum and maximum variation\ntrendresult.createOrReplaceTempView(\"trendresult\")\nspark.sql(\"select Year,variation_value as Minimum_Crime_Trend from trendresult where variation_value=(select min(variation_value) from trendresult)\").show\nspark.sql(\"select Year,variation_value as Maximum_Crime_Trend from trendresult where variation_value=(select max(variation_value) from trendresult )\").show\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"79d17c00-3c97-4838-8a5f-30326c008ba9","inputWidgets":{},"title":"Problem 5: Are there any trends in the crimes being committed?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n//Problem 6: In which locations crimes are being committed frequently? \n\nspark.sql(\"select count(*) as Crime_count,`Location Description` from crime_table where `Location Description` is not null group by `Location Description` order by Crime_count desc\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"30728583-5775-42ca-afc6-4a3a60c94e21","inputWidgets":{},"title":"Problem 6: In which locations crimes are being committed frequently?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// Problem 7: Are there certain high crime locations for certain crime?\n\n// Step 1: Creating an intermediate dataframe by grouping the columns 'Primary Type' and 'Location Description' of the 'crime_table'. It has one more column 'crime_count' representing the number of records in the respective groups i.e; actually the number of occurences of the 'Primary Type' in the respective locations.\nval tempCrimeDF=spark.sql(\"SELECT `Primary Type`, `Location Description`, COUNT(*) AS crime_count FROM crime_table GROUP BY `Primary Type`,`Location Description` ORDER BY `Primary Type`, crime_count DESC\")\n\ntempCrimeDF.createOrReplaceTempView(\"temp_table1\")\n\nspark.sql(\"SELECT t1.`Primary Type`,t1.`Location Description`,t1.crime_count FROM temp_table1 t1 INNER JOIN (SELECT `Primary Type`, MAX(crime_count) AS max_crime_count FROM temp_table1 GROUP BY `Primary Type`) AS t2 ON t1.`Primary Type`=t2.`Primary Type` AND t1.crime_count=t2.max_crime_count ORDER BY crime_count DESC\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"793d0c29-e522-4552-987c-210b1cdd2d18","inputWidgets":{},"title":"Problem 7: Are there certain high crime locations for certain crime?"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n\n// Problem 8: In which locations the frequent crimes are being committed?\n\nspark.sql(\"select `Primary Type`,`Location Description`,count(*) as Crime_count from crime_table where `Location Description` is not null and `Primary Type` in (select `Primary Type` from crime_table group by `Primary Type` having count(*) in (select count(*) as count from crime_table group by `Primary Type` order by count desc limit 5)) group by `Primary Type`,`Location Description` order by `Primary Type`,Crime_count desc \").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b7d6004e-efd8-458b-ab40-e24463f9ba53","inputWidgets":{},"title":"Problem 8: In which locations the frequent crimes are being committed?"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Chicago_crime_data_analysis","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2997028956679803}},"nbformat":4,"nbformat_minor":0}
